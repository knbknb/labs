---
layout: page
title: Hierarchical Models
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

## Hierarchical Models

In this section, we use the mathematical theory which describes an approach that has become widely applied in the analysis of high-throughput data. The general idea is to build a _hierachichal model_ with two levels. One level describes variability across samples/units, and the other describes variability across features. This is similar to the baseball example in which the first level described variability across players and the second described the randomness for the success of one player. The first level of variation is accounted for by all the models and approaches we have described here, for example the model that leads to the t-test. The second level provides power by permitting us to "borrow" information from all features to inform the inference performed on each one. 

Here we describe one specific case that is currently the most widely
used approach to inference with gene expression data. It is the model
implemented by the `limma` Bioconductor package. This idea has been
adapted to develop methods for other data types such as RNAseq by, for example,
[edgeR](http://www.ncbi.nlm.nih.gov/pubmed/19910308) and
[DESeq2](http://www.ncbi.nlm.nih.gov/pubmed/25516281). This package
provides an alternative to the t-test that greatly improves power by
modeling the variance. While in the baseball example we modeled averages, here we model variances. Modelling variances requires more advanced math, but the concepts are practically the same. We motivate and demonstrate the approach with
an example. 

Here is a volcano showing effect sizes and p-value from applying a t-test to data from an experiment running six replicated samples with 16 genes artificially made to be different in two groups of three samples each. These 16 genes are the only genes for which the alternative hypothesis is true. In the plot they are shown in blue.

```{r volcano-plot,message=FALSE, fig.cap="Volcano plot for t-test comparing two groups. Spiked-in genes are denoted with blue. Among the rest of the genes, those with p-value < 0.01 are denoted with red."}
#source("http://bioconductor.org/biocLite.R")
#biocLite("SpikeInSubset")
suppressMessages(library(SpikeInSubset))  ##Available from Bioconductor
data(rma95)
library(genefilter)
fac <- factor(rep(1:2,each=3))
tt <- rowttests(exprs(rma95),fac)

smallp <- with(tt, p.value < .01)

spike <- rownames(rma95) %in% colnames(pData(rma95))

cols <- ifelse(spike,"dodgerblue",ifelse(smallp,"red","black"))

with(tt, plot(-dm, -log10(p.value), cex=.8, pch=16,
     xlim=c(-1,1), ylim=c(0,4.5),
     xlab="difference in means",
     col=cols))
abline(h=2,v=c(-.2,.2), lty=2)
```

##### Hierarchical Models in Practice Exercises #1
What proportion of genes with a p-value < 0.01 (no multiple comparison correction) are not part of the artificially added (false positive)?

```{r}
rn.smp <- rownames(tt[which(smallp),])
rn.spike <- rownames(tt[which(spike),])
length(setdiff(rn.smp, rn.spike))/length(rn.smp)

```

We cut-off the range of the y-axis at 4.5, but there is one blue point with a p-value smaller than $10^{-6}$. Two findings stand out from this plot. The first is that only one of the positives would be found to be significant with a standard 5% FDR cutoff:

```{r}
sum( p.adjust(tt$p.value,method = "BH")[spike] < 0.05)
```

This of course has to do with the low power associated with a sample size of three in each group. The second finding is that if we forget about inference and simply rank the genes based on the size of the t-statistic, we obtain many false positives in any rank list of size larger than 1. For example, six of the top 10 genes ranked by t-statistic are false positives. 

```{r}
table( top50=rank(tt$p.value)<= 10, spike) #t-stat and p-val rank is the same
```

#####  Hierarchical Models in Practice Exercises #2 

Now compute the within group sample standard deviation for each gene (you can use group 1). Based on the p-value < 0.01 cut-off, split the genes into true positives, false positives, true negatives and false negatives. Create a boxplot comparing the sample SDs for each group. Which of the following best described the box-plot?
```{r}
group1.pvals <- tt[1:3,c("p.value")]
group1 <- exprs(rma95)[,1:3]
group1.sd <- apply(group1, 1, sd)
tp <- rownames(tt[which(smallp & spike),])
fp <- rownames(tt[which(smallp & !spike),])
tn <- rownames(tt[which(!smallp & !spike),])
fn <- rownames(tt[which(!smallp & spike),])

group1.tp <- group1[rownames(group1) %in% tp,]
group1.fp <- group1[rownames(group1) %in% fp,]
group1.tn <- group1[rownames(group1) %in% tn,]
group1.fn <- group1[rownames(group1) %in% fn,]

group1.tp.sd <- apply(group1.tp, 1, sd)
group1.fp.sd <- apply(group1.fp, 1, sd)
group1.tn.sd <- apply(group1.tn, 1, sd)
group1.fn.sd <- apply(group1.fn, 1, sd)
#par(mfrow=c(1,1))
par(mfrow=c(1,4))

boxplot(group1.tp.sd, main="TP")
boxplot(group1.fp.sd, main="FP")
boxplot(group1.tn.sd, main="TN")
boxplot(group1.fn.sd, main="FN")

```

Rafa's solution:
```{r}
par(mfrow=c(1,1))
y <- exprs(rma95)
sds <- rowSds(y[,1:3])
index <- paste0( as.numeric(spike), as.numeric(tt$p.value<0.01))
index <- factor(index,levels=c("11","01","00","10"),labels=c("TP","FP","TN","FN"))
boxplot(split(sds,index))

```


In the "volcano-plot"" we notice that these false positives are mostly genes for which the effect size is relatively small, implying that the estimated standard error is small. We can confirm this with a plot:

```{r pval_versus_sd, fig.cap="p-values versus standard deviation estimates."}
tt$s <- apply(exprs(rma95), 1, function(row) 
  sqrt(.5 * (var(row[1:3]) + var(row[4:6]) ) ) )
with(tt, plot(s, -log10(p.value), cex=.8, pch=16,
              log="x",xlab="estimate of standard deviation",
              col=cols))

```

Here is where a hierarchical model can be useful. If we can make an assumption about the distribution of these variances across genes, then we can improve estimates by "adjusting" estimates that are "too small" according to this distribution. In a previous section we described how the F-distribution approximates the distribution of the observed variances.

$$
s^2 \sim s_0^2 F_{d,d_0}
$$

Because we have thousands of data points, we can actually check this assumption and also estimate the parameters $s_0$ and $d_0$. This particular approach is referred to as empirical Bayes because it can be described as using data (empirical) to build the prior distribution (Bayesian approach). 

Now we apply what we learned with the baseball example to the standard error estimates. As before we have an observed value for each gene $s_g$, a sampling distribution as a prior distribution. We can therefore compute a posterior distribution for the variance $\sigma^2_g$ and obtain the posterior mean. You can see the details of the derivation in [this paper](http://www.ncbi.nlm.nih.gov/pubmed/16646809).

$$
\mbox{E}[\sigma^2_g \mid s_g] = \frac{d_0 s_0^2 + d s^2_g}{d_0 + d}
$$

As in the baseball example, the posterior mean _shrinks_ the observed variance $s_g^2$ towards the global variance $s_0^2$ and the weights depend on the sample size through the degrees of freedom $d$ and, in this case, the shape of the prior distribution through $d_0$. 

In the plot above we can see how the variance estimate _shrink_ for 40 genes (code not shown):


```{r shrinkage, fig.cap="Illustration of how estimates shrink towards the prior expectation. Forty genes spanning the range of values were selected.",message=FALSE, echo=FALSE}
library(limma)
fit <- lmFit(rma95, model.matrix(~ fac))
colnames(coef(fit))
ebfit <- ebayes(fit)



n <- 40
qs <- seq(from=0,to=.2,length=n)
idx <- sapply(seq_len(n),function(i) which(as.integer(cut(tt$s^2,qs)) == i)[1])
idx <- idx[!is.na(idx)]
par(mar=c(5,5,2,2))
plot(1,1,xlim=c(0,.21),ylim=c(0,1),type="n",
     xlab="variance estimates",ylab="",yaxt="n")
axis(2,at=c(.1,.9),c("before","after"),las=2)
segments((tt$s^2)[idx],rep(.1,n),
         ebfit$s2.post[idx],rep(.9,n))
```

Make a plot of the original new hierarchical models based estimate versus the sample based estimate.

```{r}
sampleSD = fit$sigma
posteriorSD = sqrt(ebfit$s2.post)

plot(sampleSD, posteriorSD, ylim=c(0, max(posteriorSD)))
abline(0,1)
abline(h=0.12,v=0.12, lty=2)

# Rafa:
        # LIM = range( c(posteriorSD,sampleSD))
# plot(sampleSD, posteriorSD,ylim=LIM,xlim=LIM)
# abline(0,1)
# abline(v=sqrt(fit$s2.prior))
```
*Which best describes what the hierarchical modelling approach does?*

Answer: It Moves all the estimates of standard deviation closer to 0.12. 


An important aspect of this adjustment is that genes having a sample standard deviation close to 0 are no longer close to 0 (the shrink towards $s_0$ ). We can now create a version of the t-test that instead of the sample standard deviation uses this posterior mean or "shrunken" estimate of the variance. We refer to these as _moderated_ t-tests. Once we do this, the improvements can be seen clearly in the volcano plot:

```{r volcano-plot2, message=FALSE, fig.cap="Volcano plot for moderated t-test comparing two groups. Spiked-in genes are denoted with blue. Among the rest of the genes, those with p-value < 0.01 are denoted with red.",message=FALSE}
library(limma)
fit <- lmFit(rma95, model.matrix(~ fac))
ebfit <- ebayes(fit)
limmares <- data.frame(dm=coef(fit)[,"fac2"], p.value=ebfit$p.value[,"fac2"])
with(limmares, plot(dm, -log10(p.value),cex=.8, pch=16,
     col=cols,xlab="difference in means",
     xlim=c(-1,1), ylim=c(0,5)))
abline(h=2,v=c(-.2,.2), lty=2)
```

The number of false positives in the top 10 is now reduced to 2. 

```{r}
table( top50=rank(limmares$p.value)<= 10, spike) 
```

Generally, what proportion of genes with a p-value < 0.01 (no multiple comparison correction) are not part of the artificially added (false positives)?

```{r}
pvals_mod = ebfit$p.value[,2]
index = which(pvals_mod < 0.01) 
print (mean( !spike[index] ))
```

